from playwright.sync_api import sync_playwright
import sys
import json
import time
from urllib.parse import urlparse
import re

# --- ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ ---
MAX_PAGES_TO_SCAN = 30
# ### FIX: Ð”Ð¾Ð±Ð°Ð²Ð¸Ð»Ð¸ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÑƒ Ð¿ÐµÑ€ÐµÐ´ ÑÐ±Ð¾Ñ€Ð¾Ð¼ ÐºÑƒÐº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ JS ÑƒÑÐ¿ÐµÐ» Ð¾Ñ‚Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ
COOKIE_WAIT_TIME = 3 

CMP_SELECTORS = [
    ".cky-btn-accept", "button.cky-btn-accept", "[data-cky-tag='accept-button']",
    "#cookie_action_close_header", ".cli_action_button", "#wt-cli-accept-all-btn",
    "#onetrust-accept-btn-handler", "#CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll",
    ".cc-btn.cc-allow", ".cc-btn.cc-dismiss", "button[data-testid='uc-accept-all-button']",
    ".qc-cmp2-summary-buttons button:first-child", "#didomi-notice-agree-button",
    ".truste-button1", "#ccc-notify-accept", ".cmplz-accept", ".iubenda-cs-accept-btn",
    ".ms-cookie-banner-button", "#accept-cookies", "#cookie-accept", ".accept-cookies-button",
    # Ð§Ð°ÑÑ‚Ð¾ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ð½Ð¾Ð²Ñ‹Ðµ:
    "button[id*='accept']", "button[class*='accept']", "a[class*='accept']"
]

ACCEPT_PATTERNS = [
    re.compile(r"accept\s+all", re.I), re.compile(r"allow\s+all", re.I),
    re.compile(r"accept", re.I), re.compile(r"allow", re.I), re.compile(r"agree", re.I),
    re.compile(r"got\s+it", re.I), re.compile(r"okay", re.I), re.compile(r"consent", re.I),
    re.compile(r"Ð¿Ñ€Ð¸Ð½ÑÑ‚ÑŒ\s+Ð²Ñ[ÐµÑ‘]", re.I), re.compile(r"Ð¿Ñ€Ð¸Ð½ÑÑ‚ÑŒ", re.I),
    re.compile(r"ÑÐ¾Ð³Ð»Ð°Ñ(ÐµÐ½|Ð½Ð°|Ð¸Ñ‚ÑŒÑÑ)", re.I), re.compile(r"Ñ€Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ", re.I),
    re.compile(r"Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾", re.I), re.compile(r"Ð´Ð°,\s+Ñ\s+ÑÐ¾Ð³Ð»Ð°ÑÐµÐ½", re.I),
    re.compile(r"akzeptieren", re.I), re.compile(r"zustimmen", re.I),
    re.compile(r"accepter", re.I), re.compile(r"tout\s+accepter", re.I)
]

def handle_banner(page):
    # (Ð’Ð°ÑˆÐ° Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹, Ð¾Ð½Ð° Ñ…Ð¾Ñ€Ð¾ÑˆÐ°Ñ)
    def check_context(context, context_name="Main"):
        for selector in CMP_SELECTORS:
            try:
                # ### FIX: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ waitFor, ÐµÑÐ»Ð¸ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ Ð¿Ð¾ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹
                if context.locator(selector).first.is_visible():
                    btn = context.locator(selector).first
                    print(f"ðŸŽ¯ Banner found by ID ({context_name}): {selector}", file=sys.stderr)
                    btn.click()
                    time.sleep(1.5)
                    return True
            except: pass

        for pattern in ACCEPT_PATTERNS:
            try:
                btn = context.get_by_role("button", name=pattern).first
                if btn.is_visible():
                    print(f"ðŸ“ Banner button found by Regex ({context_name}): {pattern.pattern}", file=sys.stderr)
                    btn.click()
                    time.sleep(1.5)
                    return True
            except: pass
            
            try:
                element = context.get_by_text(pattern).first
                if element.is_visible():
                    print(f"âš ï¸ Banner text found (fallback) ({context_name}): {pattern.pattern}", file=sys.stderr)
                    element.click(force=True)
                    time.sleep(1.5)
                    return True
            except: pass
        return False

    if check_context(page, "Main Page"): return True
    for frame in page.frames:
        if frame == page.main_frame: continue
        try:
            if check_context(frame, "iFrame"): return True
        except: pass
    return False

def get_internal_links(page, base_domain, current_url):
    # (Ð’Ð°ÑˆÐ° Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹)
    links_found = set()
    try:
        hrefs = page.evaluate("""() => {
            return Array.from(document.querySelectorAll('a')).map(a => a.href);
        }""")
        for href in hrefs:
            href = href.split('#')[0].rstrip('/')
            if not href: continue
            parsed = urlparse(href)
            if parsed.netloc == base_domain:
                if not any(href.lower().endswith(ext) for ext in ['.pdf', '.jpg', '.png', '.css', '.js', '.zip']):
                     links_found.add(href)
        del hrefs 
        return links_found
    except:
        return set()

def scan(start_url):
    unique_cookies = {} 
    queue = [start_url]
    visited = set()
    base_domain = urlparse(start_url).netloc
    pages_scanned = 0

    with sync_playwright() as p:
        # ### FIX 1: ÐœÐ°ÑÐºÐ¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾Ð´ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ñ„Ð»Ð°Ð³Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Chrome
        browser = p.chromium.launch(
            headless=True, # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ False, ÐµÑÐ»Ð¸ Ð²ÑÐµ Ñ€Ð°Ð²Ð½Ð¾ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚!
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox"
            ]
        )
        
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080},
            locale="en-US,en;q=0.9" # ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ°Ð¹Ñ‚Ñ‹ ÑÐ¼Ð¾Ñ‚Ñ€ÑÑ‚ Ð½Ð° Ð»Ð¾ÐºÐ°Ð»ÑŒ
        )
        
        # ### FIX 2: Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² webdriver
        context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)

        page = context.new_page()

        try:
            while queue and pages_scanned < MAX_PAGES_TO_SCAN:
                url = queue.pop(0)
                if url in visited: continue
                
                visited.add(url)
                pages_scanned += 1
                
                try:
                    # ### FIX 3: WaitUntil = NetworkIdle
                    # Ð–Ð´ÐµÐ¼, Ð¿Ð¾ÐºÐ° Ð·Ð°ÐºÐ¾Ð½Ñ‡Ð°Ñ‚ÑÑ ÑÐµÑ‚ÐµÐ²Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ (Ð·Ð°Ð³Ñ€ÑƒÐ·ÑÑ‚ÑÑ ÑÐºÑ€Ð¸Ð¿Ñ‚Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸)
                    # Ð•ÑÐ»Ð¸ ÑÐ°Ð¹Ñ‚ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ñ‹Ð¹, networkidle Ð¼Ð¾Ð¶ÐµÑ‚ Ð¾Ñ‚Ð²Ð°Ð»Ð¸Ð²Ð°Ñ‚ÑŒÑÑ Ð¿Ð¾ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ñƒ, Ñ‚Ð¾Ð³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ 'load'
                    try:
                        page.goto(url, wait_until="networkidle", timeout=30000)
                    except:
                        # Ð•ÑÐ»Ð¸ networkidle Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð·Ð° 30 ÑÐµÐº, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ domcontentloaded
                        page.goto(url, wait_until="domcontentloaded", timeout=30000)

                    if pages_scanned <= 3:
                        handle_banner(page)

                    # Ð¡ÐºÑ€Ð¾Ð»Ð»
                    page.mouse.wheel(0, 3000)
                    time.sleep(COOKIE_WAIT_TIME) # Ð”Ð°ÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ ÑÐºÑ€Ð¸Ð¿Ñ‚Ð°Ð¼ Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÑƒÐºÐ¸
                    
                    # 4. Ð¡Ð±Ð¾Ñ€ ÐºÑƒÐºÐ¸
                    current_cookies = context.cookies()
                    for c in current_cookies:
                        unique_cookies[c['name']] = c

                    # ### FIX 4: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° LocalStorage (Ð¸Ð½Ð¾Ð³Ð´Ð° Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ‚Ð°Ð¼)
                    local_storage_data = page.evaluate("() => JSON.stringify(localStorage)")
                    # Ð•ÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾, Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚

                    new_links = get_internal_links(page, base_domain, url)
                    for link in new_links:
                        if link not in visited and link not in queue:
                            queue.append(link)
                    
                except Exception as e:
                    print(f"Error scanning {url}: {e}", file=sys.stderr)

            return {
                "url": start_url,
                "cookies": list(unique_cookies.values()),
                "cookie_count": len(unique_cookies),
                "pages_scanned": pages_scanned,
                "visited_urls": list(visited)
            }

        except Exception as e:
            return {"error": str(e)}
        finally:
            browser.close()

if __name__ == "__main__":
    url = sys.argv[1] if len(sys.argv) > 1 else "https://example.com"
    data = scan(url)
    print(json.dumps(data, indent=2))